# k-최근접 이웃 회귀

도미와 빙어를 구분하는 모델을 개발하였지만

추가적으로 무게로 예측할 수 있냐는 추가적인 과제가 등장

농어의 길이, 높이, 두께를 측적한 데이터가 있으므로 추가적으로 무게를 예측할 수 있지 않을까라는 의견이 나왔다

일단은 농어의 무게를 정확하게 측정한 샘플이 필요하다

지도 학습 알고리즘은 분류와 회귀로 나누는데

회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제

(예를 들어 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제, 정해진 클래스가 없고 임의의 수치를 출력)

## k-최근접 이웃 분류 알고리즘

예측하려는 샘플에 가장 가까운 샘플 k개를 선택한다

이 샘플들의 클래스를 확인하여 다수 클래스를 새로운 샘플의 클래스로 예측한다

샘플이 3개라고 가정한다면 샘플 2개는 사각형, 샘플 하나는 삼각형으로 확인되면 새로운 샘플은 사각형으로 나온다

## k-최근접 이웃 회귀

샘플에 가장 가까운 샘플 k개를 선택한다

회귀이기 때문에 이웃한 샘플의 타깃은 어떤 클래스가 아니라 임의의 수치

샘플을 예측하는 가장 간단한 방법은 가장 가까운 아웃들의 수치들의 평균을 구하면 된다

가까운 이웃들의 수치가 60, 80, 100이라면 샘플의 예측 타깃값은 80이 된다

## 데이터 준비

훈련 데이터 준비를 위해서 농어의 길이만 있어도 무게를 잘 예측한다고 가정한다

농어의 길이가 특성이고 무게가 타깃이 된다

```python
import numpy as np

perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

import matplotlib.pyplot as plt
plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

test_array = np.array([1, 2, 3, 4])
print(test_array.shape) # (4,)

test_array = test_array.reshape(2, 2)
print(test_array.shape) # (2, 2)

test_array = test_array.reshape(2, 3)
print(test_array.shape) # error


train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
# 크기에 -1을 지정하면 다른 차원을 채우고 남은 원소에 맞게 차원을 지정하라는 의미
print(train_input.shape, test_input.shape) # (42, 1) (14, 1)

```

## 결정계수(R^2)

사이킷런에서 k-최근접 이웃 회귀 알고리즘을 구현한 클래스는 `KNeighborsRegressor`

```python
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()

# k-최근접 이웃 회귀 모델을 훈련
knr.fit(train_input, train_target)

# 테스트 세트의 점수
print(knr.score(test_input, test_target)) # 0.992809406101064

from sklearn.metrics import mean_absolute_error

# 테스트 세트에 대한 예측을 만듭니다
test_prediction = knr.predict(test_input)

# 테스트 세트에 대한 평균 절댓값 오차를 계산합니다
mae = mean_absolute_error(test_target, test_prediction)
print(mae) # 19.157142857142862 => 19g
```

## 과대적합 vs 과소적합

테스트 세트에서의 점수는 아래와 같다

```python
print(knr.score(train_input, train_target)) # 0.9698823289099254
```

훈련 세트에서 점수가 좋았는데 테스트 세트에서 점수가 굉장히 나쁘다면 모델이 훈련 세트에 `과대적합`

훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 낮은 경우 `과소적합`

앞서 평가한 것은 훈련세트보다 테스트 세트의 점수가 더 높기 때문에 과소적합에 해당한다



과소적합에 해당될 경우 모델을 복잡하게 만들면 된다

훈련 세트에 더 잘 맞게 만들면 테스트 세트의 점수는 조금 낮아진다 모델을 복잡하게 만드는 방법은 이웃의 개수 k를 줄이는 것

이웃의 개수를 줄이면 훈련 세트에 있는 국지적인 패턴에 민감해지고, 이웃의 개수를 늘리면 데이터 전반에 있는 일반적인 패턴에 따를 것

이때까지 사이킷런의 k-최근접 이웃 알고리즘의 기본 k 값은 5인데 3으로 낮추면 된다

```python
knr.knr.n_neighbors = 3

knr.fit(train_input, train_target)
print(knr.score(train_input, train_target))
```

테스트 점수는 아래와 같다

```python
print(knr.score(test_input, test_target))
```
